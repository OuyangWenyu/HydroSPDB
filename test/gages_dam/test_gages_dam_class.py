import copy
import csv
import unittest
from matplotlib import pyplot as plt
import torch
import seaborn as sns
from data import GagesConfig, GagesSource
from data.config import cfg, cmd, update_cfg
from data.data_input import save_datamodel, GagesModel, save_result, load_result, _basin_norm
from data.gages_input_dataset import GagesDamDataModel, choose_which_purpose
from data.nid_input import NidModel
from explore.stat import statError
from hydroDL.master.master import master_train, master_test
import numpy as np
import os
import pandas as pd

from utils import unserialize_json


class MyTestCase(unittest.TestCase):
    """historical data assimilation"""

    def setUp(self) -> None:
        """before all of these, natural flow model need to be generated by config.ini of gages dataset, and it need
        to be moved to right dir manually """
        self.config_file = copy.deepcopy(cfg)
        args = cmd(sub="dam/exp60",
                   gage_id_file="/mnt/data/owen411/code/hydro-anthropogenic-lstm/example/output/gages/basic/exp37/3557basins_ID_NSE_DOR.csv")
        update_cfg(self.config_file, args)
        self.config_data = GagesConfig(self.config_file)
        self.random_seed = self.config_file.RANDOM_SEED
        self.test_epoch = self.config_file.TEST_EPOCH
        self.gpu_num = self.config_file.CTX
        self.train_mode = self.config_file.TRAIN_MODE
        self.sites_id = self.config_file.GAGES.gageIdScreen
        print("train and test in large-dor basins: \n")

    def test_explore_dor_dam_num(self):
        config_data = self.config_data
        dor_2 = 0.1
        source_data_dor2 = GagesSource.choose_some_basins(config_data,
                                                          config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_largedam = source_data_dor2.all_configs['flow_screen_gage_id']
        sites_id = np.intersect1d(np.array(self.sites_id), np.array(sites_id_largedam)).tolist()
        norsto = source_data_dor2.read_attr(sites_id, ["STOR_NOR_2009"], is_return_dict=False)
        dam_num = source_data_dor2.read_attr(sites_id, ["NDAMS_2009"], is_return_dict=False)
        df = pd.DataFrame({"GAGE_ID": sites_id, "STOR_NOR": norsto.flatten(), "DAM_NUM": dam_num.flatten()})
        # df.to_csv(os.path.join(source_data_dor1.all_configs["out_dir"], '3557basins_NORSTOR.csv'),
        #           quoting=csv.QUOTE_NONNUMERIC, index=None)
        sns.distplot(df["DAM_NUM"], bins=50)
        plt.show()
        df.to_csv(os.path.join(source_data_dor2.all_configs["out_dir"], '1185largedor_basins_NORSTOR_DAMNUM.csv'),
                  quoting=csv.QUOTE_NONNUMERIC, index=None)

    def test_explore_damcls_datamodel(self):
        config_data = self.config_data
        sites_id_dict = unserialize_json(
            "/mnt/data/owen411/code/hydro-anthropogenic-lstm/example/data/gages/nid/test/dam_main_purpose_dict.json")
        sites_id = list(sites_id_dict.keys())
        source_data_dor1 = GagesSource.choose_some_basins(config_data,
                                                          config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          sites_id=sites_id)
        norsto = source_data_dor1.read_attr(sites_id, ["STOR_NOR_2009"], is_return_dict=False)
        df = pd.DataFrame({"GAGE_ID": sites_id, "STOR_NOR": norsto.flatten()})
        # df.to_csv(os.path.join(source_data_dor1.all_configs["out_dir"], '3557basins_NORSTOR.csv'),
        #           quoting=csv.QUOTE_NONNUMERIC, index=None)
        df.to_csv(os.path.join(source_data_dor1.all_configs["out_dir"], '2909basins_NORSTOR.csv'),
                  quoting=csv.QUOTE_NONNUMERIC, index=None)

    def test_explore_(self):
        config_data = self.config_data
        sites_id_dict = unserialize_json(
            "/mnt/data/owen411/code/hydro-anthropogenic-lstm/example/data/gages/nid/test/dam_main_purpose_dict.json")
        sites_id = list(sites_id_dict.keys())
        source_data_dor1 = GagesSource.choose_some_basins(config_data,
                                                          config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          sites_id=sites_id)
        nse_all = pd.read_csv(
            "/mnt/data/owen411/code/hydro-anthropogenic-lstm/example/output/gages/basic/exp37/3557basins_ID_NSE_DOR.csv",
            dtype={0: str})
        sites_ids = nse_all["GAUGE ID"].values
        idx = [i for i in range(len(sites_ids)) if sites_ids[i] in sites_id]
        df = pd.DataFrame({"GAGE_ID": sites_id, "NSE": nse_all["NSE"].values[idx]})
        # df.to_csv(os.path.join(source_data_dor1.all_configs["out_dir"], '3557basins_NORSTOR.csv'),
        #           quoting=csv.QUOTE_NONNUMERIC, index=None)
        df.to_csv(os.path.join(source_data_dor1.all_configs["out_dir"], '2909basins_NSE.csv'),
                  quoting=csv.QUOTE_NONNUMERIC, index=None)

    def test_dam_train(self):
        with torch.cuda.device(0):
            nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
            gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
            gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
            gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
            for i in range(0, gage_main_dam_purpose_unique.size):
                df = GagesModel.load_datamodel(self.config_data.data_path["Temp"], gage_main_dam_purpose_unique[i],
                                               data_source_file_name='data_source.txt',
                                               stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                               forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                               f_dict_file_name='dictFactorize.json',
                                               var_dict_file_name='dictAttribute.json',
                                               t_s_dict_file_name='dictTimeSpace.json')
                new_temp_dir = os.path.join(df.data_source.data_config.model_dict["dir"]["Temp"],
                                            gage_main_dam_purpose_unique[i])
                new_out_dir = os.path.join(df.data_source.data_config.model_dict["dir"]["Out"],
                                           gage_main_dam_purpose_unique[i])
                df.update_datamodel_dir(new_temp_dir, new_out_dir)
                master_train(df)

    def test_damcls_test_datamodel(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "allnonref_85-05_nan-0.1_00-1.0")
        data_model_train = GagesModel.load_datamodel(data_dir,
                                                     data_source_file_name='data_source.txt',
                                                     stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                                     forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                                     f_dict_file_name='dictFactorize.json',
                                                     var_dict_file_name='dictAttribute.json',
                                                     t_s_dict_file_name='dictTimeSpace.json')
        data_model_test = GagesModel.load_datamodel(data_dir,
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')

        gages_model_train = GagesModel.update_data_model(self.config_data, data_model_train)
        df = GagesModel.update_data_model(self.config_data, data_model_test,
                                          train_stat_dict=gages_model_train.stat_dict)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        nid_input = NidModel.load_nidmodel(nid_dir, nid_file=self.nid_file,
                                           nid_source_file_name='nid_source.txt', nid_data_file_name='nid_data.shp')
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        data_input = GagesDamDataModel(df, nid_input, True, gage_main_dam_purpose)
        for i in range(gage_main_dam_purpose_unique.size):
            gages_input = choose_which_purpose(data_input, purpose=gage_main_dam_purpose_unique[i])
            save_datamodel(gages_input, gage_main_dam_purpose_unique[i], data_source_file_name='test_data_source.txt',
                           stat_file_name='test_Statistics.json', flow_file_name='test_flow',
                           forcing_file_name='test_forcing', attr_file_name='test_attr',
                           f_dict_file_name='test_dictFactorize.json', var_dict_file_name='test_dictAttribute.json',
                           t_s_dict_file_name='test_dictTimeSpace.json')

    def test_data_temp_test_damcls(self):

        with torch.cuda.device(0):
            nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
            gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
            gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
            gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
            for i in range(0, gage_main_dam_purpose_unique.size):
                df = GagesModel.load_datamodel(self.config_data.data_path["Temp"], gage_main_dam_purpose_unique[i],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json', flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy', attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
                new_temp_dir = os.path.join(df.data_source.data_config.model_dict["dir"]["Temp"],
                                            gage_main_dam_purpose_unique[i])
                new_out_dir = os.path.join(df.data_source.data_config.model_dict["dir"]["Out"],
                                           gage_main_dam_purpose_unique[i])
                df.update_datamodel_dir(new_temp_dir, new_out_dir)
                pred, obs = master_test(df, epoch=self.test_epoch)
                basin_area = df.data_source.read_attr(df.t_s_dict["sites_id"], ['DRAIN_SQKM'],
                                                      is_return_dict=False)
                mean_prep = df.data_source.read_attr(df.t_s_dict["sites_id"], ['PPTAVG_BASIN'],
                                                     is_return_dict=False)
                mean_prep = mean_prep / 365 * 10
                pred = _basin_norm(pred, basin_area, mean_prep, to_norm=False)
                obs = _basin_norm(obs, basin_area, mean_prep, to_norm=False)
                save_result(new_temp_dir, self.test_epoch, pred, obs)

    def test_plot_cases(self):
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        for i in range(0, gage_main_dam_purpose_unique.size):
            data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"], gage_main_dam_purpose_unique[i],
                                                   data_source_file_name='test_data_source.txt',
                                                   stat_file_name='test_Statistics.json',
                                                   flow_file_name='test_flow.npy',
                                                   forcing_file_name='test_forcing.npy', attr_file_name='test_attr.npy',
                                                   f_dict_file_name='test_dictFactorize.json',
                                                   var_dict_file_name='test_dictAttribute.json',
                                                   t_s_dict_file_name='test_dictTimeSpace.json')
            new_temp_dir = os.path.join(data_model.data_source.data_config.model_dict["dir"]["Temp"],
                                        gage_main_dam_purpose_unique[i])
            new_out_dir = os.path.join(data_model.data_source.data_config.model_dict["dir"]["Out"],
                                       gage_main_dam_purpose_unique[i])
            data_model.update_datamodel_dir(new_temp_dir, new_out_dir)
            pred, obs = load_result(new_temp_dir, self.test_epoch)
            pred = pred.reshape(pred.shape[0], pred.shape[1])
            obs = obs.reshape(pred.shape[0], pred.shape[1])
            inds = statError(obs, pred)
            inds_df = pd.DataFrame(inds)
            print(gage_main_dam_purpose_unique[i])
            print(inds_df.median(axis=0))
            print(inds_df.mean(axis=0))
            # plot_we_need(data_model, obs, pred, show_me_num=1, id_col="STAID", lon_col="LNG_GAGE", lat_col="LAT_GAGE")


if __name__ == '__main__':
    unittest.main()
