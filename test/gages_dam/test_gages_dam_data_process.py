import copy
import unittest

import torch

import definitions
from data import GagesConfig
from data.config import cfg, cmd, update_cfg
from data.data_input import save_datamodel, GagesModel, _basin_norm, save_result
from data.gages_input_dataset import GagesDamDataModel, choose_which_purpose
from data.nid_input import NidModel, save_nidinput
from hydroDL.master.master import master_train, master_test
import os

from utils import serialize_json, unserialize_json


class MyTestCase(unittest.TestCase):
    """historical data assimilation"""

    def setUp(self) -> None:
        """before all of these, natural flow model need to be generated by config.ini of gages dataset, and it need
        to be moved to right dir manually """
        # self.config_file = os.path.join(config_dir, "dam/config_exp10.ini")
        # self.subdir = r"dam/exp10"
        self.config_file = copy.deepcopy(cfg)
        args = cmd(sub="dam/exp15", quick_data=1, cache_state=0)
        update_cfg(self.config_file, args)

        self.config_data = GagesConfig(self.config_file)
        # self.nid_file = 'PA_U.xlsx'
        # self.nid_file = 'OH_U.xlsx'
        # self.nid_file = 'NID2018_U.xlsx'
        # self.test_epoch = 300

    def test_gages_dam_attr(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
        df = GagesModel.load_datamodel(data_dir,
                                       data_source_file_name='data_source.txt',
                                       stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                       forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                       f_dict_file_name='dictFactorize.json',
                                       var_dict_file_name='dictAttribute.json',
                                       t_s_dict_file_name='dictTimeSpace.json')
        # nid_input = NidModel()
        nid_input = NidModel(self.config_data.config_file)
        # nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "test")
        save_nidinput(nid_input, nid_dir, nid_source_file_name='nid_source.txt', nid_data_file_name='nid_data.shp')
        data_input = GagesDamDataModel(df, nid_input)
        serialize_json(data_input.gage_main_dam_purpose, os.path.join(nid_dir, "dam_main_purpose_dict.json"))

    def test_gages_dam_all_save(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
        data_model_train = GagesModel.load_datamodel(data_dir,
                                                     data_source_file_name='data_source.txt',
                                                     stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                                     forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                                     f_dict_file_name='dictFactorize.json',
                                                     var_dict_file_name='dictAttribute.json',
                                                     t_s_dict_file_name='dictTimeSpace.json')

        gages_model_train = GagesModel.update_data_model(self.config_data, data_model_train)
        data_model_test = GagesModel.load_datamodel(data_dir,
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')
        gages_model_test = GagesModel.update_data_model(self.config_data, data_model_test,
                                                        train_stat_dict=gages_model_train.stat_dict)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "test")
        nid_input = NidModel.load_nidmodel(nid_dir, nid_source_file_name='nid_source.txt',
                                           nid_data_file_name='nid_data.shp')
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        data_input = GagesDamDataModel(gages_model_test, nid_input, gage_main_dam_purpose)
        data_model_dam = choose_which_purpose(data_input)
        save_datamodel(data_model_dam, data_source_file_name='test_data_source.txt',
                       stat_file_name='test_Statistics.json', flow_file_name='test_flow',
                       forcing_file_name='test_forcing', attr_file_name='test_attr',
                       f_dict_file_name='test_dictFactorize.json', var_dict_file_name='test_dictAttribute.json',
                       t_s_dict_file_name='test_dictTimeSpace.json')

    def test_dam_coords_storages(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
        data_model_train = GagesModel.load_datamodel(data_dir,
                                                     data_source_file_name='data_source.txt',
                                                     stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                                     forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                                     f_dict_file_name='dictFactorize.json',
                                                     var_dict_file_name='dictAttribute.json',
                                                     t_s_dict_file_name='dictTimeSpace.json')

        gages_model_train = GagesModel.update_data_model(self.config_data, data_model_train)
        data_model_test = GagesModel.load_datamodel(data_dir,
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')
        gages_model_test = GagesModel.update_data_model(self.config_data, data_model_test,
                                                        train_stat_dict=gages_model_train.stat_dict)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "test")
        nid_input = NidModel.load_nidmodel(nid_dir, nid_source_file_name='nid_source.txt',
                                           nid_data_file_name='nid_data.shp')
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        data_input = GagesDamDataModel(gages_model_test, nid_input, gage_main_dam_purpose)
        dam_coords, dam_storages = data_input.coords_of_dams()
        serialize_json(dam_coords, os.path.join(nid_dir, "dam_points_dict.json"))
        serialize_json(dam_storages, os.path.join(nid_dir, "dam_storages_dict.json"))

    def test_dam_train(self):
        """just test for one purpose as a case"""
        with torch.cuda.device(2):
            quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
            data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
            df = GagesModel.load_datamodel(data_dir,
                                           data_source_file_name='data_source.txt',
                                           stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                           forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                           f_dict_file_name='dictFactorize.json',
                                           var_dict_file_name='dictAttribute.json',
                                           t_s_dict_file_name='dictTimeSpace.json')
            nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
            nid_input = NidModel.load_nidmodel(nid_dir, nid_file=self.nid_file, nid_source_file_name='nid_source.txt',
                                               nid_data_file_name='nid_data.shp')
            gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
            data_input = GagesDamDataModel(df, nid_input, True, gage_main_dam_purpose)
            purpose_chosen = 'C'
            gages_input = choose_which_purpose(data_input, purpose=purpose_chosen)
            master_train(gages_input)

    def test_dam_test(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
        data_model_train = GagesModel.load_datamodel(data_dir,
                                                     data_source_file_name='data_source.txt',
                                                     stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                                     forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                                     f_dict_file_name='dictFactorize.json',
                                                     var_dict_file_name='dictAttribute.json',
                                                     t_s_dict_file_name='dictTimeSpace.json')
        data_model_test = GagesModel.load_datamodel(data_dir,
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')

        gages_model_train = GagesModel.update_data_model(self.config_data, data_model_train)
        gages_model_test = GagesModel.update_data_model(self.config_data, data_model_test,
                                                        train_stat_dict=gages_model_train.stat_dict)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        nid_input = NidModel.load_nidmodel(nid_dir, nid_file=self.nid_file,
                                           nid_source_file_name='nid_source.txt', nid_data_file_name='nid_data.shp')
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        data_input = GagesDamDataModel(gages_model_test, nid_input, True, gage_main_dam_purpose)
        gages_input = choose_which_purpose(data_input)
        pred, obs = master_test(gages_input)
        basin_area = gages_input.data_source.read_attr(gages_input.t_s_dict["sites_id"], ['DRAIN_SQKM'],
                                                       is_return_dict=False)
        mean_prep = gages_input.data_source.read_attr(gages_input.t_s_dict["sites_id"], ['PPTAVG_BASIN'],
                                                      is_return_dict=False)
        mean_prep = mean_prep / 365 * 10
        pred = _basin_norm(pred, basin_area, mean_prep, to_norm=False)
        obs = _basin_norm(obs, basin_area, mean_prep, to_norm=False)
        save_result(gages_input.data_source.data_config.data_path['Temp'], self.test_epoch, pred, obs)


if __name__ == '__main__':
    unittest.main()
